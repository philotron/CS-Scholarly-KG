{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    AutoModelForSequenceClassification\n",
    ")\n",
    "from sklearn.metrics import f1_score\n",
    "import torch as nn\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate tokenizer\n",
    "\n",
    "model_checkpoint = \"allenai/scibert_scivocab_uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing function\n",
    "\n",
    "def preprocess_function_batch(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"sentence\"], \n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512,\n",
    "        #add_special_tokens=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in dataset\n",
    "\n",
    "label2id = {\"BACKGROUND\": 0, \"OBJECTIVE\": 1, \"METHODS\": 2, \"RESULTS\": 3, \"CONCLUSIONS\": 4}\n",
    "\n",
    "with open('data/train.txt') as f:\n",
    "    train_lines = f.readlines()    \n",
    "train_data = []\n",
    "for line in train_lines:\n",
    "    new_line = {\n",
    "        \"sentence\": line.split(\"\\t\")[2][:-2], \n",
    "        \"label\": label2id[line.split(\"\\t\")[1]]\n",
    "    }\n",
    "    train_data.append(new_line)\n",
    "\n",
    "with open('data/validation.txt') as f:\n",
    "    eval_lines = f.readlines()    \n",
    "eval_data = []\n",
    "for line in eval_lines:\n",
    "    new_line = {\n",
    "        \"sentence\": line.split(\"\\t\")[2][:-2], \n",
    "        \"label\": label2id[line.split(\"\\t\")[1]]\n",
    "    }\n",
    "    eval_data.append(new_line)\n",
    "\n",
    "with open('data/test.txt') as f:\n",
    "    test_lines = f.readlines()    \n",
    "test_data = []\n",
    "for line in test_lines:\n",
    "    new_line = {\n",
    "        \"sentence\": line.split(\"\\t\")[2][:-2], \n",
    "        \"label\": label2id[line.split(\"\\t\")[1]]\n",
    "    }\n",
    "    test_data.append(new_line)\n",
    "\n",
    "print(train_data[:5])\n",
    "print(eval_data[:5])\n",
    "print(test_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create huggingface datasets\n",
    "\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "eval_dataset = Dataset.from_list(eval_data)\n",
    "test_dataset = Dataset.from_list(test_data)\n",
    "\n",
    "print(train_dataset)\n",
    "print(eval_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode datasets\n",
    "\n",
    "train_encoded = train_dataset.map(preprocess_function_batch, batched=True)\n",
    "eval_encoded = eval_dataset.map(preprocess_function_batch, batched=True)\n",
    "test_encoded = test_dataset.map(preprocess_function_batch, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename to \"labels\" & calculate class weights\n",
    "\n",
    "final_train = train_encoded.rename_column(\"label\", \"labels\")\n",
    "final_eval = eval_encoded.rename_column(\"label\", \"labels\")\n",
    "final_test = test_encoded.rename_column(\"label\", \"labels\")\n",
    "\n",
    "train_df = final_train.to_pandas()\n",
    "class_weights = (1 - (train_df[\"labels\"].value_counts().sort_index() / len(train_df))).values\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted loss function (because of imbalanced classes)\n",
    "\n",
    "class WeightedLossTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        labels = inputs.get(\"labels\")\n",
    "        loss_func = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        loss = loss_func(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use f1 score as metric\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    return {\"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=5, hidden_dropout_prob=0.25)\n",
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify output directory\n",
    "\n",
    "model_output_dir = \"scibert-finetuned-abstract-classification-h6\"\n",
    "print(model_output_dir)\n",
    "\n",
    "# start TensorBoard before training to monitor progress\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir '{model_output_dir}'/runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup trainer arguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=model_output_dir,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    learning_rate=3e-5,\n",
    "    weight_decay=0.05,\n",
    "    warmup_steps=100,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=10,\n",
    "    load_best_model_at_end=True,\n",
    "    # fp16=True, \n",
    "    report_to=\"tensorboard\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=final_train,\n",
    "    eval_dataset=final_eval,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# train the model & save checkpoint\n",
    "\n",
    "trainer.train()\n",
    "model.save_pretrained(model_output_dir + \"/best_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check on test set\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "dataset_test_encoded = final_test\n",
    "test_predictions = trainer.predict(dataset_test_encoded)\n",
    "test_predictions_argmax = np.argmax(test_predictions[0], axis=1)\n",
    "test_references = np.array(final_test[\"labels\"])\n",
    "# Compute accuracy & f1\n",
    "print(\"Test Results:\")\n",
    "print(\"accuracy:\", metric.compute(predictions=test_predictions_argmax, references=test_references)[\"accuracy\"])\n",
    "print(\"f1-score:\", f1_score(test_references, test_predictions_argmax, average=\"weighted\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
