{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = ['Speech & Audio in NLP', 'Multimodality', 'Visual Data in NLP',\n",
    "                  'Structured Data in NLP', 'Programming Languages in NLP',\n",
    "                  'Natural Language Interfaces', 'Question Answering',\n",
    "                  'Dialogue Systems & Conversational Agents', 'Semantic Text Processing',\n",
    "                  'Discourse & Pragmatics', 'Representation Learning', 'Knowledge Representation',\n",
    "                  'Text Complexity', 'Semantic Search', 'Word Sense Disambiguation',\n",
    "                  'Semantic Parsing', 'Language Models', 'Semantic Similarity',\n",
    "                  'Sentiment Analysis', 'Opinion Mining', 'Stylistic Analysis',\n",
    "                  'Intent Recognition', 'Emotion Analysis', 'Aspect-based Sentiment Analysis',\n",
    "                  'Polarity Analysis', 'Syntactic Text Processing', 'Tagging', 'Morphology',\n",
    "                  'Chunking', 'Phonology', 'Text Error Correction', 'Text Segmentation',\n",
    "                  'Typology', 'Syntactic Parsing', 'Phonetics', 'Text Normalization',\n",
    "                  'Linguistics & Cognitive NLP', 'Linguistic Theories', 'Cognitive Modeling',\n",
    "                  'Psycholinguistics', 'Responsible & Trustworthy NLP', 'Responsible NLP',\n",
    "                  'Ethical NLP', 'Low-Resource NLP', 'Robustness in NLP', 'Green & Sustainable NLP',\n",
    "                  'Explainability & Interpretability in NLP', 'Reasoning', 'Textual Inference',\n",
    "                  'Commonsense Reasoning', 'Numerical Reasoning', 'Knowledge Graph Reasoning',\n",
    "                  'Machine Reading Comprehension', 'Fact & Claim Verification', 'Argument Mining',\n",
    "                  'Multilinguality', 'Cross-Lingual Transfer', 'Machine Translation',\n",
    "                  'Code-Switching', 'Information Retrieval', 'Indexing', 'Document Retrieval',\n",
    "                  'Text Classification', 'Passage Retrieval', 'Information Extraction & Text Mining',\n",
    "                  'Coreference Resolution', 'Text Clustering', 'Named Entity Recognition',\n",
    "                  'Event Extraction', 'Open Information Extraction', 'Term Extraction',\n",
    "                  'Topic Modeling', 'Relation Extraction', 'Text Generation',\n",
    "                  'Data-to-Text Generation', 'Question Generation', 'Dialogue Response Generation',\n",
    "                  'Captioning', 'Paraphrassing', 'Paraphrasing', 'Text Style Transfer',\n",
    "                  'Code Generation', 'Summarization', 'Speech Recognition']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtopics = ['Passage Retrieval',\n",
    " 'Chunking',\n",
    " 'Text Error Correction',\n",
    " 'Named Entity Recognition',\n",
    " 'Text Normalization',\n",
    " 'Dialogue Systems & Conversational Agents',\n",
    " 'Psycholinguistics',\n",
    " 'Machine Translation',\n",
    " 'Relation Extraction',\n",
    " 'Captioning',\n",
    " 'Emotion Analysis',\n",
    " 'Opinion Mining',\n",
    " 'Knowledge Representation',\n",
    " 'Language Models',\n",
    " 'Text Complexity',\n",
    " 'Open Information Extraction',\n",
    " 'Semantic Search',\n",
    " 'Cross-Lingual Transfer',\n",
    " 'Linguistic Theories',\n",
    " 'Tagging',\n",
    " 'Code Generation',\n",
    " 'Fact & Claim Verification',\n",
    " 'Commonsense Reasoning',\n",
    " 'Aspect-based Sentiment Analysis',\n",
    " 'Speech Recognition',\n",
    " 'Coreference Resolution',\n",
    " 'Speech & Audio in NLP',\n",
    " 'Low-Resource NLP',\n",
    " 'Machine Reading Comprehension',\n",
    " 'Question Generation',\n",
    " 'Term Extraction',\n",
    " 'Event Extraction',\n",
    " 'Text Classification',\n",
    " 'Question Answering',\n",
    " 'Cognitive Modeling',\n",
    " 'Stylistic Analysis',\n",
    " 'Discourse & Pragmatics',\n",
    " 'Code-Switching',\n",
    " 'Document Retrieval',\n",
    " 'Data-to-Text Generation',\n",
    " 'Programming Languages in NLP',\n",
    " 'Semantic Similarity',\n",
    " 'Word Sense Disambiguation',\n",
    " 'Dialogue Response Generation',\n",
    " 'Ethical NLP',\n",
    " 'Text Segmentation',\n",
    " 'Typology',\n",
    " 'Argument Mining',\n",
    " 'Morphology',\n",
    " 'Textual Inference',\n",
    " 'Responsible & Trustworthy NLP',\n",
    " 'Text Clustering',\n",
    " 'Knowledge Graph Reasoning',\n",
    " 'Representation Learning',\n",
    " 'Structured Data in NLP',\n",
    " 'Intent Recognition',\n",
    " 'Summarization',\n",
    " 'Paraphrasing',\n",
    " 'Green & Sustainable NLP',\n",
    " 'Visual Data in NLP',\n",
    " 'Explainability & Interpretability in NLP',\n",
    " 'Numerical Reasoning',\n",
    " 'Semantic Parsing',\n",
    " 'Robustness in NLP',\n",
    " 'Indexing',\n",
    " 'Phonology',\n",
    " 'Phonetics',\n",
    " 'Syntactic Parsing',\n",
    " 'Topic Modeling',\n",
    " 'Polarity Analysis',\n",
    " 'Text Style Transfer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "import pandas as pd\n",
    "\n",
    "class Neo4jApp:\n",
    "\n",
    "    def __init__(self):\n",
    "        uri = os.getenv(\"uri\", \"neo4j://0.0.0.0:7687\")\n",
    "        user = os.getenv(\"user\", \"neo4j\")\n",
    "        password = os.getenv(\"password\", \"neo4j-connect\")\n",
    "        self.driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.close()\n",
    "\n",
    "    def get_children_nodes(self, topic_name):\n",
    "        with self.driver.session() as session:\n",
    "            subtopics = session.execute_read(self._get_children_nodes, topic_name)\n",
    "            return subtopics\n",
    "\n",
    "    def get_parent_nodes(self, topic_name):\n",
    "        with self.driver.session() as session:\n",
    "            subtopics = session.execute_read(self._get_parent_nodes, topic_name)\n",
    "            return subtopics\n",
    "        \n",
    "    def get_topic_definition(self, topic_name):\n",
    "        with self.driver.session() as session:\n",
    "            topic = session.execute_read(self._get_topic_definition, topic_name)\n",
    "            return topic\n",
    "        \n",
    "   # Just for back up if we want to be more dynamic \n",
    "    def get_nlp_taxonomy(self):\n",
    "        with self.driver.session() as session:\n",
    "            taxonomy = session.execute_read(self._get_nlp_taxonomy)\n",
    "            return taxonomy\n",
    "        \n",
    "    def get_total_nodes(self):\n",
    "        with self.driver.session() as session:\n",
    "            total_nodes = session.execute_read(self._get_total_nodes)\n",
    "            return total_nodes\n",
    "\n",
    "    def get_articles_in_topic(self, topic_name):\n",
    "        with self.driver.session() as session:\n",
    "            articles = session.execute_read(self._get_articles_in_topic, topic_name)\n",
    "            return articles\n",
    "    \n",
    "        \n",
    "    def get_all_articles(self):\n",
    "        with self.driver.session() as session:\n",
    "            articles = session.execute_read(self._get_all_articles)\n",
    "            return articles\n",
    "    \n",
    "    def get_all_topics(self):\n",
    "        with self.driver.session() as session:\n",
    "            topics = session.execute_read(self._get_all_topics)\n",
    "            return topics\n",
    "        \n",
    "    @staticmethod\n",
    "    def _get_total_nodes(tx):\n",
    "        query = (\n",
    "            \"MATCH (n) \"\n",
    "            \"RETURN count(n) as total_nodes \"\n",
    "        )\n",
    "        total_nodes = tx.run(query)\n",
    "        total_nodes = total_nodes.single()[0]\n",
    "        return total_nodes\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_topic_definition(tx, topic_name: str):\n",
    "        query = (\n",
    "            \"MATCH (n:FieldOfStudy {label: $topic_name }) \"\n",
    "            \"RETURN n.description as description \"\n",
    "            \"LIMIT 100 \"\n",
    "        )\n",
    "        topic_definition = tx.run(query, topic_name=topic_name)\n",
    "        topic_definition = str([row[0] for row in topic_definition])[2:-3]\n",
    "        if topic_definition == \"\":\n",
    "            result_str = f\"We don't have an available definition for {topic_name}. Try to search for topics related to Natural Language Processing. You could also ask for the taxonomy.\"\n",
    "        else: \n",
    "            result_str = f\"The available definition of {topic_name} is: \\n{topic_definition}\"\n",
    "        \n",
    "        return result_str\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_children_nodes(tx, topic_name):\n",
    "        query = (\n",
    "            \"MATCH (n:FieldOfStudy {label: $topic_name }) -[]->(m:FieldOfStudy) \"\n",
    "            \"WHERE n.level < m.level \"\n",
    "            \"RETURN m.label \"\n",
    "            \"LIMIT 100 \"\n",
    "        )\n",
    "        children_nodes = tx.run(query, topic_name=topic_name)\n",
    "        children_nodes = [row[0] for row in children_nodes]\n",
    "        if len(children_nodes) == 0:\n",
    "            return f\"{topic_name} apparently doesn't have subtopics.\"\n",
    "        children_nodes = ('\\n  - ').join(children_nodes)\n",
    "        if children_nodes == \"\":\n",
    "            return \"I don't know this topic. Try to search for topics related to Natural Language Processing. You could also ask for the taxonomy.\"\n",
    "        result_str = f\"Subtopics of {topic_name} are \\n- {children_nodes}. \\nYou can ask me for the definitions of these terms.\"\n",
    "        \n",
    "        return result_str\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_parent_nodes(tx, topic_name):\n",
    "        query = (\n",
    "            \"MATCH (n:FieldOfStudy {label: $topic_name }) -[]->(m:FieldOfStudy) \"\n",
    "            \"WHERE n.level > m.level \"\n",
    "            \"RETURN m.label \"\n",
    "            \"LIMIT 100 \"\n",
    "        )\n",
    "        parent_nodes = tx.run(query, topic_name=topic_name)\n",
    "        parent_nodes = [row[0] for row in parent_nodes]\n",
    "        if len(parent_nodes) == 0:\n",
    "            return f\"{topic_name} apparently doesn't have parents.\"\n",
    "        parent_nodes = ('\\n  - ').join(parent_nodes)\n",
    "        if parent_nodes == \"\":\n",
    "            return \"I don't know this topic. Try to search for topics related to Natural Language Processing. You could also ask for the taxonomy.\"\n",
    "        result_str = f\"Parents of {topic_name} are \\n- {parent_nodes}. \\nYou can ask me for the definitions of these terms.\"\n",
    "        \n",
    "        return result_str\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_articles_in_topic(tx, topic_name):\n",
    "        query = (\n",
    "            \"MATCH (n:FieldOfStudy {label: $topic_name }) -[:IS_STUDIED_IN]->(p:Publication) \"\n",
    "            \"RETURN ID(p) AS id, p.publicationTitle AS title, p.embedding AS embedding, p.publicationAbstract AS abstract, p.tldr AS tldr \"\n",
    "        )\n",
    "        results = tx.run(query, topic_name=topic_name)\n",
    "        df = pd.DataFrame(results, columns=['id', 'title', 'embedding', 'abstract', 'tldr'])\n",
    "        #if len(results) == 0:\n",
    "            #sreturn f\"{topic_name} apparently doesn't have articles.\"\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_all_topics(tx):\n",
    "        query = (\n",
    "            \"MATCH (n:FieldOfStudy) \"\n",
    "            \"RETURN n.label AS topic_name \"\n",
    "        )\n",
    "        results = tx.run(query)\n",
    "        df = pd.DataFrame([dict(record) for record in results], columns=['topic_name'])\n",
    "        return df\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_all_articles(tx):\n",
    "        query = (\n",
    "            \"MATCH (p:Publication) \"\n",
    "            \"RETURN p.publicationTitle AS title, p.embedding AS embedding, p.publicationAbstract AS abstract, p.tldr AS tldr \"\n",
    "        )\n",
    "        results = tx.run(query)\n",
    "        df = pd.DataFrame([dict(record) for record in results], columns=['id', 'title', 'embedding', 'abstract', 'tldr'])\n",
    "        return df\n",
    "\n",
    "    # Just for back up if we want to be more dynamic \n",
    "    @staticmethod\n",
    "    def _get_nlp_taxonomy(tx):\n",
    "        bullet_point0 = '\\n   -- '\n",
    "        bullet_point1 = '\\n-- '\n",
    "        level0 = 'Natural Language Processing'\n",
    "        level1 = tx.run(\"match (n:FieldOfStudy {label: 'Natural Language Processing'})-[]->(m:FieldOfStudy) where m.level = [1] return m.label limit 100\")\n",
    "        level1 = [row[0] for row in level1]\n",
    "        for i, topic in enumerate(level1):\n",
    "            subtopics_level_2 = tx.run(\"match (n:FieldOfStudy {label: $topic})-[]->(m:FieldOfStudy) where m.level = [2] return m.label limit 100\", topic=topic)\n",
    "            level1[i] += (bullet_point0 + '\\n   -- '.join([row[0] for row in subtopics_level_2]))\n",
    "        taxonomy = ('- ' + level0 + '\\n '+ bullet_point1 + ('\\n-- ').join(level1))\n",
    "        return taxonomy[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_embedding(df):\n",
    "    df['embedding'] = df['embedding'].apply(lambda x: x[1:-1].split(','))\n",
    "    df['embedding'] = df['embedding'].apply(lambda x: [float(i) for i in x])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.cluster as cl\n",
    "import sklearn.metrics as met\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def embed(df, distance_threshold=50, max_cluster_size=10, decay_rate=0.40, base=0.5):\n",
    "    def fit_cluster(X, threshold):\n",
    "        hier = cl.AgglomerativeClustering(n_clusters=None, distance_threshold=threshold, linkage='ward')\n",
    "        hier.fit(X)\n",
    "        return hier.labels_\n",
    "\n",
    "    def subcluster(df, cluster_col, level, max_cluster_size, current_distance):\n",
    "        new_cluster_col = f'cluster_level_{level}'\n",
    "        updates = []\n",
    "\n",
    "        for cluster_id in df[cluster_col].unique():\n",
    "            sub_df = df[df[cluster_col] == cluster_id]\n",
    "            if len(sub_df) <= max_cluster_size:\n",
    "                updates.extend([(index, (cluster_id, 0)) for index in sub_df.index])\n",
    "            else:\n",
    "                X_sub = np.array(sub_df['embedding'].tolist())\n",
    "                sub_labels = fit_cluster(X_sub, current_distance)\n",
    "                updates.extend([(index, (cluster_id, sub_label)) for index, sub_label in zip(sub_df.index, sub_labels)])\n",
    "\n",
    "        update_df = pd.DataFrame(updates, columns=['index', new_cluster_col]).set_index('index')\n",
    "        df = df.merge(update_df, left_index=True, right_index=True, how='left')\n",
    "\n",
    "        # Apply exponential decay to the distance threshold\n",
    "        next_distance = current_distance * (decay_rate ** (base ** level))\n",
    "\n",
    "        if df[new_cluster_col].apply(lambda x: x[1]).nunique() > 1:\n",
    "            df = subcluster(df, new_cluster_col, level + 1, max_cluster_size, next_distance)\n",
    "        return df\n",
    "\n",
    "    # Start with initial clustering\n",
    "    X = np.array(df['embedding'].tolist())\n",
    "    initial_labels = fit_cluster(X, distance_threshold)\n",
    "    df['cluster_level_0'] = initial_labels\n",
    "\n",
    "    # Kick off the recursive subclustering with adjusted initial distance\n",
    "    initial_distance_adjusted = distance_threshold * decay_rate\n",
    "    return subcluster(df, 'cluster_level_0', 1, max_cluster_size, initial_distance_adjusted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "class CustomTfidfVectorizer(TfidfVectorizer):\n",
    "    def __init__(self, key_terms=None, stop_words='english', use_idf=True, min_df=.001, max_df=0.8, ngram_range=(2,5)):\n",
    "        super(CustomTfidfVectorizer, self).__init__(stop_words=stop_words, use_idf=use_idf, min_df=min_df, max_df=max_df, ngram_range=ngram_range)\n",
    "        self.key_terms = key_terms if key_terms is not None else {}\n",
    "\n",
    "    def fit(self, raw_documents, y=None):\n",
    "        super().fit(raw_documents, y=y)\n",
    "        for term, value in self.key_terms.items():\n",
    "            if term in self.vocabulary_:\n",
    "                index = self.vocabulary_[term]\n",
    "                self._tfidf.idf_[index] = value  # Manually adjust the idf\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycorenlp import StanfordCoreNLP\n",
    "import pandas as pd\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "\n",
    "def lemmatize_and_tag(text):\n",
    "    if pd.isna(text):\n",
    "        return [(\"empty\", \"empty\")]\n",
    "    try:\n",
    "        result = nlp.annotate(text, properties={\n",
    "            'annotators': 'lemma, pos',\n",
    "            'outputFormat': 'json'\n",
    "        })\n",
    "\n",
    "        if isinstance(result, str):\n",
    "            result = json.loads(result)\n",
    "\n",
    "        lemmas_pos = [(token['lemma'], token['pos']) for sentence in result['sentences'] for token in sentence['tokens']]\n",
    "    except AssertionError as e:\n",
    "        print('Error:', e)\n",
    "        print('Failed text:', text)\n",
    "        print('Failed result:', result)\n",
    "        raise\n",
    "    return lemmas_pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Assuming CustomTfidfVectorizer is defined elsewhere and works similarly to sklearn's TfidfVectorizer\n",
    "\n",
    "def calculate_tf_idf_for_subclusters(df, idf_name, tf_name, filter_list=[], key_terms=None):\n",
    "    custom_vectorizer = CustomTfidfVectorizer(\n",
    "        stop_words=\"english\", \n",
    "        use_idf=True, \n",
    "        min_df=.01, \n",
    "        max_df=0.8, \n",
    "        ngram_range=(2,5),  # Adjust ngram_range to capture up to trigrams\n",
    "        key_terms=key_terms\n",
    "    )\n",
    "    # Fit the vectorizer to the idf_name column\n",
    "    custom_vectorizer.fit([\" \".join([word for word, pos in lst]) for lst in df[idf_name]])\n",
    "\n",
    "    cluster_labels = {}  # Map from cluster_id to label\n",
    "\n",
    "    #max_level = int(df.columns.str.extract('cluster_level_(\\d+)')[0].astype(float).max())\n",
    "\n",
    "    cluster_level_cols = df.columns[df.columns.str.startswith('cluster_level_')]\n",
    "\n",
    "    # print(\"cluster_level_cols\", cluster_level_cols)\n",
    "\n",
    "    # Extract the numeric part of these column names and convert to int\n",
    "    cluster_levels = cluster_level_cols.str.extract('cluster_level_(\\d+)')[0].astype(int)\n",
    "    max_level = cluster_levels.max()\n",
    "\n",
    "    # print(\"cluster_levels\", cluster_levels)\n",
    "\n",
    "    # print(\"max_level\", max_level)\n",
    "\n",
    "    for level in range(max_level + 1):\n",
    "        cluster_col = f'cluster_level_{level}'\n",
    "        tag_col = f'cluster_tag_level_{level}'\n",
    "        df[tag_col] = \"\"\n",
    "\n",
    "        used_labels = set()\n",
    "\n",
    "        for cluster_id in df[cluster_col].unique():\n",
    "            \n",
    "\n",
    "            subcluster_df = df[df[cluster_col] == cluster_id]\n",
    "            subcluster_nouns = [word for lst in subcluster_df[tf_name] for word, pos in set(lst) if pos.startswith('NN') and word not in filter_list]\n",
    "\n",
    "            if subcluster_nouns:\n",
    "                tfidf_matrix = custom_vectorizer.transform([' '.join(subcluster_nouns)])\n",
    "                sorted_indices = np.argsort(tfidf_matrix.toarray()).flatten()[::-1]  # Sort scores descending\n",
    "                labels_attempted = 0\n",
    "\n",
    "                for index in sorted_indices:\n",
    "                    term = custom_vectorizer.get_feature_names_out()[index]\n",
    "                    potential_label = f\"{term}\"\n",
    "\n",
    "                    if potential_label not in used_labels:\n",
    "                        used_labels.add(potential_label)\n",
    "                        cluster_labels[cluster_id] = potential_label\n",
    "                        break\n",
    "                    else:\n",
    "                        labels_attempted += 1\n",
    "                        # Generate a new potential label by appending secondary terms if the primary is already used\n",
    "                        if labels_attempted < len(sorted_indices):  # Ensure there are more terms to attempt\n",
    "                            next_best_index = sorted_indices[labels_attempted]\n",
    "                            next_best_term = custom_vectorizer.get_feature_names_out()[next_best_index]\n",
    "                            potential_label = f\"{term} {next_best_term}\"\n",
    "                            if potential_label not in used_labels:\n",
    "                                used_labels.add(potential_label)\n",
    "                                cluster_labels[cluster_id] = potential_label\n",
    "                                break\n",
    "\n",
    "                df.loc[df[cluster_col] == cluster_id, tag_col] = cluster_labels.get(cluster_id, \"\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(df_sorted, topic_names):\n",
    "    #df_sorted['title_abstract'] = df_sorted['title']+df_sorted['abstract']\n",
    "    #df_sorted['lemmatized_and_tagged_full'] = df_sorted['title_abstract'].apply(lemmatize_and_tag)\n",
    "    df_sorted['lemmatized_and_tagged_title'] = df_sorted['title'].apply(lemmatize_and_tag)\n",
    "\n",
    "    key_terms_example = {\"quantum entanglement\": 0.5, \"machine learning\": 0.6}\n",
    "    df_sorted = calculate_tf_idf_for_subclusters(df_sorted, 'lemmatized_and_tagged_title', 'lemmatized_and_tagged_title', topic_names)    \n",
    "\n",
    "    #df_sorted = calculate_tf_idf(df_sorted, 'lemmatized_and_tagged_full', 'lemmatized_and_tagged_title', topic_names)\n",
    "    return df_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "n4j = Neo4jApp()\n",
    "for topic in subtopics:\n",
    "    try:\n",
    "        #topic_df = fix_embedding(topic_df)\n",
    "        \n",
    "        # Directory path\n",
    "        directory = './output'\n",
    "\n",
    "        # Check if the directory exists\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "\n",
    "        formatted_topic = topic.replace(' ', '_').lower()\n",
    "\n",
    "        # Now save the DataFrame\n",
    "\n",
    "        #topic_df.to_csv(os.path.join(directory, 'clustered_'+formatted_topic+'.csv'), index=False)        \n",
    "\n",
    "        file_path = os.path.join(directory, 'clustered_' + formatted_topic + '.csv')\n",
    "\n",
    "        # Check if the file already exists\n",
    "        if not os.path.exists(file_path):            \n",
    "            topic_df = n4j.get_articles_in_topic(topic)\n",
    "\n",
    "            #topic_df = fix_embedding(topic_df)\n",
    "\n",
    "\n",
    "            topic_df = embed(topic_df)\n",
    "            word_list = topic.split()\n",
    "            topic_df = tfidf(topic_df, word_list)\n",
    "\n",
    "            # If the file does not exist, save the DataFrame to CSV\n",
    "            topic_df.to_csv(file_path, index=False)\n",
    "            print(\"Done\", topic)\n",
    "        else:\n",
    "            pass\n",
    "            # If the file exists, print a message\n",
    "            #print(\"File already exists:\", topic)\n",
    "        #topic_df.to_csv('./output/clustered_'+topic+'.csv', index=False)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"error\", topic)\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
